% Theoretical background
 %if the chapter heading starts close to bottom of the page, force a line break and remove the vertical vspace
\vspace{21.5pt}
\chapter{Literature Review: Fuzzing as a Testing Technique}~\label{sec:sec_fuzzing}
This chapter turns its attention to fuzz testing, a specific type of software
testing. It starts with an introduction to fuzz testing, then moves on to cover
its history and development, giving a thorough background of how it has evolved.
The next parts discuss why fuzz testing is used, how it works, and what parts of
software it focuses on. The chapter goes into detail about the elements of fuzz
testing, including \gls{fuzzers}, test harnesses, and monitoring tools. It ends with a
discussion on the effectiveness of fuzz testing, especially in embedded projects.

\section{Fuzzing: An Introduction, History, Successes, and Components}
``Fuzzy testing, often referred to as fuzzing'', has become an essential technique in
identifying vulnerabilities during software testing. This approach uses iterative and
random generation of test inputs to a target program, aiming to uncover software anomalies.
During the \gls{sut} phase, tools termed ``fuzzers'' generate significant amounts
of both valid and invalid data. These data sets are introduced to the \gls{fuzz_target}, and the
\gls{fuzzers} observe and record any anomalies during the testing process~\cite{klees2018evaluating}~\cite{li2018fuzzing}.

Fuzzing has become more common because of its ability to detect vulnerabilities\cite{thiel2008exposing}
that might be missed by traditional testing methods. Additionally, fuzzing can be employed
to evaluate software on various platforms and configurations, which is useful
for intricate applications. The random and iterative characteristics of fuzzing make it suitable for
evaluating software with complex behavior patterns~\cite{klees2018evaluating}.

While fuzzing is acknowledged for vulnerability discovery,
it complements, rather than replaces, other testing methods.
It often operates in tandem with other techniques, ensuring
a well-rounded software testing strategy. As software systems become more complex
and varied, fuzz testing continues to be a vital tool for maintaining the
security and stability of current software systems~\cite{kim2011efficient}.

The fuzz testing method, a structured approach to identify software vulnerabilities,
follows several distinct stages. Initially, the \gls{fuzz_target} is determined, which
might be an entire application or just a specific file or library within it~\cite{segedyfuzz}.
Next, the input data that is sent from the client to the target system is
determined.

The subsequent stage involves test case generation, producing various blends of
valid and malformed data in forms like binary or files. Next, the fuzzers
execute the target program using the previously created inputs, halting after a
designated timeout. In this part, fuzzers note any anomalies or unexpected outcomes.

The penultimate stage, termed the analysis phase, requires the fuzzers to review the
outcomes from the previous step. The concluding stage determines if any observed anomalies
during the program's operation signal potential vulnerabilities. This evaluation discerns
whether the observed behavior indicates a genuine security concern or a simple error.

%\clearpage
The Figure:~\ref{fig:fuzzy_testing_phases_1} describes different stages of fuzzing.

\begin{figure}[h]
        \centering
        \AltText{Flow diagram of Fuzzing Stages.
        It starts with `Start', followed by `Identify the Target',
        `Identify the Input to Mutate', `Generate Fuzzed Data',
        and `Execute Fuzzed Data'.
        Based on execution, if exceptions are found,
        it leads to `Report Bug' and then `End'.
        If no exceptions, the process loops back to
        `Identify the Input to Mutate'.}{\adjustbox{width=\textwidth}{\includegraphics{fuzzy_testing_phases_1}}}
        \caption{Fuzzing Stages~\cite{segedyfuzz}~\cite{9742291}}\label{fig:fuzzy_testing_phases_1}
\end{figure}

Fuzz testing creates a large number of test cases using both correct and
incorrect data. This method can uncover software vulnerabilities that
traditional testing might miss. Moreover, analyzing the results of fuzz testing
helps software developers find and fix security problems before attackers can
take advantage of them.

\subsection{History and Evolution of Fuzzy Testing}
The concept of ``fuzzing'' or ``fuzz testing'', although a relatively new term
in automated testing techniques, was first introduced by Barton Miller in 1988
during a class project.~\cite{takanen2009fuzzing}. Initially,
fuzzing was perceived as an ad-hoc or random testing method used within
mission-critical applications~\cite{WhatisaM24:online}. Over time, however, it
has developed into a specialized technique for the automated generation and
testing of extensive sets of input values for software systems~\cite{bohme2020fuzzing}.

The advent of the first open-source fuzz testing framework in 2007~\cite{takanen2009fuzzing}
marked a significant evolution in formalizing fuzzing as an approach to software testing.
This led to the creation of a myriad of additional frameworks and tools, expanding the
application of fuzz testing across a diverse set of software systems.

Since its inception, fuzzing has considerably broadened its capacities.
Initially, it was utilized mainly to uncover memory corruption bugs. However,
as discussed by \Citeauthor{bohme2020fuzzing}, its functionality has
evolved over time to include the identification of a diverse range of
software vulnerabilities~\cite{bohme2020fuzzing}. \Citeauthor{takanen2009fuzzing} and \Citeauthor{vidas2019fuzzy}
explain that the flexibility and customization capabilities of modern fuzzing
frameworks enable their application across various software layers,
making fuzzing a potent tool for identifying potential weak spots in
software systems~\cite{takanen2009fuzzing}.

Fuzzing techniques have evolved beyond mere bug detection, becoming an essential
tool for vulnerability discovery and, potentially, exploitation.
As indicated by \citeauthor{beaman2022fuzzing}, fuzzing can be
systematically applied to uncover software vulnerabilities, which
may subsequently become exploitable avenues for security
attacks~\cite{beaman2022fuzzing}. The subsequent section, referenced as
Section~\ref{par:success_of_fuzzing}, describes the accomplishments and
vulnerabilities revealed by fuzzing techniques over time.

\subsection{Successes Of Fuzzing}~\label{par:success_of_fuzzing}
Since its beginning in the late 1980s, fuzz testing has been a highly effective
method, uncovering longstanding bugs and vulnerabilities, and thus enhancing the
quality and security of software. Over the years, it has been applied in a
variety of ways, as detailed in Section~\ref{par:target_categories}, have been successfully fuzzed,
revealing critical security vulnerabilities that could have been exploited by malicious actors.

The Table:~\ref{tab:vulnerabilities_examples} showcases examples of vulnerabilities discovered by
fuzzing tools and the affected components.

\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{@{}>{\raggedright\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{3cm}X@{}}
\toprule
\textbf{Fuzzing Tool} & \textbf{CVE ID} & \textbf{Description and Affected Component} \\
\midrule
AFL & CVE-2014-0160 & Heartbleed vulnerability in OpenSSL, affecting the TLS heartbeat extension~\cite{durumeric2014matter} \\
\addlinespace
AFL & CVE-2014-6271 & Shellshock vulnerability in the Bash shell, allowing remote code execution~\cite{shetty2018shellshock} \\
\addlinespace
libFuzzer & CVE-2016-1839  & Heap buffer overflow in ImageIO, affecting image decoding in macOS and iOS \\
\addlinespace
syzkaller & CVE-2017-2636  & Double fetch vulnerability in the Linux kernel, allowing privilege escalation~\cite{wang2017double} \\
\addlinespace
go-fuzz & CVE-2016-3959  & Denial of service vulnerability in Go's standard library, affecting the HTTP/2 implementation \\
\addlinespace
Honggfuzz & CVE-2017-15650  & Heap buffer overflow in Poppler, affecting PDF rendering~\cite{haller2013dowsing} \\
\bottomrule
\end{tabularx}
\caption{Examples of Vulnerabilities Discovered by Fuzzing Tools}
\label{tab:vulnerabilities_examples}
\end{table}

%\pagebreak

%\clearpage
The Table:~\ref{tab:bugs_found_overview} provides a reported bugs found by
different fuzzing tools in various projects over the years.

\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{@{}>{\raggedright\arraybackslash}p{1cm}>{\raggedright\arraybackslash}p{2cm}>{\raggedright\arraybackslash}p{3cm}X@{}}
\toprule
\textbf{Year} & \textbf{Organization/Researchers} & \textbf{Fuzzing Tool/Project} & \textbf{Achievements} \\
\midrule
1999 & OUSPG & PROTOS & Found security vulnerabilities in Voice over IP (VoIP) implementations~\cite{roning1999protos} \\
\addlinespace
2007 & Microsoft & SAGE & Found critical vulnerabilities in Windows and Office products and had a remarkable impact at Microsoft~\cite{godefroid2012sage} \\
\addlinespace
2010 & Adobe & NA & Vulnerabilities uncovered by fuzzing in Adobe Flash, Reader, and Acrobat~\cite{AdobeFla64:online} \\
\addlinespace
2016 & ForAllSecure & Mayhem & Competed in DARPA's Cyber Grand Challenge, identified, patched vulnerabilities in real-time~\cite{“Mayhem”62:online} \\
\addlinespace
2019 & Microsoft & Project OneFuzz & Used for fuzzing Azure Cloud~\cite{GitHubmi60:online} \\
\addlinespace
2023 & Cross platform & AFL & More than 2000 bugs found in open source projects, including OpenSSL, PHP, Python, and others~\cite{american20:online} \\
\addlinespace
2023 & Cross-platform & libFuzzer & More than 1000 bugs found in Chromium, OpenSSL, LibreOffice, and other projects~\cite{libFuzze17:online} \\
\addlinespace
2023 & Linux kernel developers & syzkaller & More than 800 bugs found in the Linux kernel~\cite{syzkalle20:online} \\
\addlinespace
2023 & Go community & go-fuzz & 200+ bugs found in Go programming language and related projects~\cite{GitHubdv6:online} \\
\addlinespace
2023 & Cross-platform & Honggfuzz & Bugs found in various projects, including Google's Android and Chrome, and others~\cite{GitHubgo17:online} \\
\addlinespace
2023 & Facebook & Facebook's Sapienz & Automated testing of Android apps, leading to crash fixes and improvements~\cite{SapienzI13:online} \\
\addlinespace
2023 & Google & ClusterFuzz & More than 25000 bugs found in Google and Chrome~\cite{ClusterF78:online} \\
\addlinespace
2023 & Google & OSS-Fuzz & more than 36,000 bugs in over 550 open-source projects~\cite{ClusterF78:online} \\
\bottomrule
\end{tabularx}
\caption{Overview of Bugs Found by Fuzzing Tools in Various Projects}
\label{tab:bugs_found_overview}
\end{table}

%\clearpage

The Table:~\ref{tab:embedded_fuzzing_success} includes fuzzing success in embedded systems and IoT devices
specifying whether the fuzzing tools are open-source or closed-source:

\begin{table}[h!]
\centering
\begin{tabularx}{\textwidth}{@{}>{\raggedright\arraybackslash}p{0.5cm}>{\raggedright\arraybackslash}p{1.5cm}>{\raggedright\arraybackslash}p{3.8cm}X>{\raggedright\arraybackslash}p{1.2cm}@{}}
\toprule
\textbf{No.} & \textbf{Fuzzing Tool} & \textbf{Target Systems} & \textbf{Achievements} & \textbf{Source} \\
\midrule
1 & AFLNet & FTP, HTTP, and SMTP implementations in IoT devices & Uncovered vulnerabilities and improved security in IoT devices\cite{lin2020aflnet} & Open-source \\
\addlinespace
2 & IoTcube & IoT devices, like smart home appliances, medical devices, and industrial IoT devices & Identified and addressed security issues in IoT devices\cite{kim2019iotcube} & Closed-source \\
\addlinespace
3 & FirmFuzz & Firmware in IoT devices including IP cameras and routers & Discovered 7 previously undisclosed vulnerabilities across 6 different devices\cite{srivastava2019firmfuzz} & Open-source \\
\addlinespace
4 & FUZZY & Zigbee implementations in IoT devices & Identified new security vulnerabilities in Zigbee, a widely used wireless communication protocol for IoT devices\cite{vidas2019fuzzy} & Closed-source \\
\addlinespace
5 & Atheris & Python programs, including embedded systems using Micro-Python & Found different kinds of weaknesses, like buffer overflows and memory leaks, in embedded systems that use Python\cite{atheris2020} & Open-source \\
\bottomrule
\end{tabularx}
\caption{Fuzzing Success in Embedded Projects}
\label{tab:embedded_fuzzing_success}
\end{table}

%\clearpage

\subsection{Why, How and What to Fuzz}

Fuzzing provides significant insights into the security, robustness,
and resilience of software systems, especially those that interact with inputs
from untrusted sources. While it does not make any guarantees about the
reliability of inputs or transforms an untrusted source into a trusted one,
it helps identify problematic inputs that could cause the software system to
crash or behave
unexpectedly~\cite{bohme2020fuzzing}~\cite{beaman2022fuzzing}~\cite{vidas2019fuzzy}~\cite{WhatisFu63:online}.

It's crucial to emphasize that fuzz testing doesn't necessarily affirm the
correctness of complex software programs. Instead, it contributes to the
understanding of the system's behavior under diverse and unexpected input
conditions. Particularly for software playing a critical role in larger
systems or applications, fuzz testing can highlight unforeseen
issues~\cite{fuzzinga40:online}~\cite{demott2006evolving}~\cite{WhatisFu63:online}.

Moreover, fuzz testing aids in assessing the software system's stability under
stress. By exposing the software to high volumes of varied inputs, this
technique can identify potential performance or stability issues, arising
from heavy usage or stress conditions~\cite{demott2006evolving}.

However, it's crucial to recognize that fuzz testing is not a cure-all for every
software defect. It's a targeted tool designed to find errors and
vulnerabilities that may not be directly connected to the software's
requirements or intended functions. While fuzz testing is helpful in identifying
specific issues like memory leaks~\cite{shahriar2014testing},
address corruptions~\cite{muench2018you}, and
buffer overflows~\cite{godefroid2020fuzzing}, it should not be the only method
used for software testing. It is better used in addition to other testing
methods such as functional, unit, integration, and system testing to ensure a
more thorough check of the software's quality and
security~\cite{pietikainen2016steps}\cite{UnitTest25:online}\cite{WhatisFu63:online}.

To leverage fuzzing effectively, careful planning is essential to identify
\gls{fuzz_target} for testing. Notably, identifying and defining entry points
for fuzzing in large and complex applications can be challenging but essential.
These entry points, including file inputs, network inputs, and command
line inputs, help fuzz testers concentrate their efforts on the areas
most susceptible to potential issues~\cite{oehlert2005violating}.
Thus, the meticulous selection of
fuzz testing targets is instrumental to the test's success, underpinning
thorough testing of the most vulnerable software components.

Below given examples applications targets where fuzzing has been successful~\cite{fuzzingw44:online}.\label{par:target_categories}

\begin{itemize}
        \item Databases: SQLite~\cite{fu2022griffin}
        \item Text editors: VIM~\cite{FuzzingV8:online}, OpenOffice~\cite{Presenta60:online}
        \item Media codecs: audio, video, raster and vector images~\cite{thiel2008exposing}~\cite{rhodenfuzzing}
        \item Linux OS kernels, drivers~\cite{schumilo2017kafl}~\cite{corina2017difuze}
        \item Parsers: xml, pdf~\cite{wang2017skyfire}
        \item Crypto libraries: OpenSSL~\cite{opensslR89:online}, LibreSSL~\cite{GitHubli9:online}
        \item Browsers: Chrome~\cite{Fuzztest23:online}, Firefox~\cite{Fuzzing—21:online}
        \item Network protocols~\cite{GitHubnc82:online}, scanners~\cite{SnapFuzz27:online}
\end{itemize}


\section{Fuzzing Process, Components, and Various Approaches}\label{sec:fuzzing_methods}
Fuzz testing, a dynamic method for improving software reliability and security,
has become increasingly important with more complex software systems. This
section details the fuzz testing process, its evolution, and methodologies.

\subsection{Test Case Generation: From Basic to Advanced Techniques}
The process of fuzzing begins with the generation of numerous inputs
designed to provoke the program's failure. These inputs can be in various forms,
including different file formats, binary executables, or network commands
~\cite{mcnally2012fuzzing}~\cite{bohme2020fuzzing}~\cite{manes2019art}.
Creating broken inputs that can trigger a program to fail is a
significant challenge. This is addressed through mutation-based
~\cite{miller2007analysis}~\cite{lyu2022ems} and
generation-based generators~\cite{pang2023generation}.

Mutation-based fuzzing represents an evolution from primitive fuzzing techniques
that relied heavily on generating inputs using random bytes.
This mutation-based fuzzing technique employs existing input data,
referred to as a test \gls{corpus}, and modifies it to produce new test data~\cite{miller2007analysis}.
These mutation algorithms can vary significantly, with some utilizing
stochastic modeling to concentrate mutations on specific parts of the input~\cite{lyu2022ems}~\cite{miller2007analysis}.

Generation-based generators, conversely, create entirely new inputs
for fuzzers, serving as a crucial tool in the
fuzzing process~\cite{li2018fuzzing}~\cite{miller2007analysis}~\cite{wang2017skyfire}.

\subsection{Execution and Monitoring: Coverage-Guided Fuzzing and Sanitizers}

Once the inputs are created, they are input into the program, and the fuzz
testing goes on until the program either crashes or stops responding. During
this phase, tools like sanitizers~\cite{GitHubgo55:online}~\cite{osterlund2020parmesan}
are used to monitor for crashes and exceptions.
These tools can identify specific system signals, crashes, and rule violations.


\subsection*{Sanitizers}

Sanitizers play a vital role in the fuzzing process, aiding in the detection of
bugs that may not necessarily lead to a crash. However, their operation is
resource-intensive, increasing both CPU~\cite{WhatisaC78:online} and
RAM~\cite{WhatisRA11:online} usage. Thus, the application of
sanitizers should be judiciously managed to prevent excessive resource consumption.

\begin{itemize}

\item \textit{Address Sanitizers (ASAN)~\cite{AddressS43:online}:} These tools
are essential for finding vulnerabilities related to memory corruption, such as
use-after-free, NULL pointer dereferences, and buffer overflows~\cite{haller2013dowsing}.

\item \textit{Memory Sanitizers (MSAN)~\cite{MemorySa64:online}:} They are made
to spot unauthorized reading of uninitialized memory, like when a local variable
is defined and read before it's given a value~\cite{stepanov2015memorysanitizer}.

\item \textit{Undefined Behavior Sanitizers (UBSAN)~\cite{Undefine50:online}:} These sanitizers are instrumental
in pinpointing instances of undefined behavior as per the C and C++ standards,
such as when the outcome of an operation exceeds the permissible limit of a data type.
\end{itemize}

Coverage-guided fuzzing~\cite{jaaskela2016genetic} is a subset of feedback-based
fuzzing that specifically uses code coverage as its feedback mechanism. It represents another integral element of
the execution and monitoring phase. This technique involves monitoring the extent and
sections of the \acrshort{sut} that are traversed by the inputs during the
fuzzing campaign. Compiler instrumentation, employed by fuzzers like
AFL/AFL++~\cite{257204} or libFuzzer~\cite{libFuzze17:online}, the usual method
for gathering this data is by using external function calls that are added at
particular stages during compilation. These calls transmit information to the
fuzzer whenever an edge or block is reached~\cite{libFuzze17:online}~\cite{bohme2016coverage}~\cite{257204}.


The Figure:~\ref{fig:feedback-based-fuzzing} depicts a simple visual representation of the
feedback based fuzzing.

% \begin{figure}[ht]
% \centering
% \AltText{Feedback Based Fuzzing}{\includegraphics[width=8.5cm, height=5.5cm]{feedback-based-fuzzing}}
% \caption{Feedback Based Fuzzing\cite{TheMagic36:online}\cite{LucianoR49:online}}\label{fig:feedback-based-fuzzing}
% \end{figure}

\begin{figure}[h]
        \centering
        \AltText{Diagram of Feedback Based Fuzzing showing a
        fuzzer sending inputs to, and receiving coverage
        feedback from, the System Under Test, forming a
        continuous feedback loop.}{\adjustbox{width=\textwidth}{\includegraphics{feedback-based-fuzzing}}}
        \caption{Feedback Based Fuzzing~\cite{TheMagic36:online}\cite{LucianoR49:online}}\label{fig:feedback-based-fuzzing}
\end{figure}

\subsection{Analysis: Discovering and Filtering Bugs}
The analysis stage is centered on uncovering the underlying cause of issues
unearthed during the monitoring phase. This stage comprises the bug
detector~\cite{liang2018fuzzing}~\cite{bekrar2012taint}, a vital element in fuzzers
aimed at pinpointing potential bugs,
and the bug filter~\cite{peng2018t}~\cite{bekrar2012taint}~\cite{chen2013taming},
which segregates non-security related bugs
from the aggregate of reported bugs. The bug detector aggregates and scrutinizes the
stack traces resulting from the crashes and errors induced by the test
inputs, while the bug filter aids in distilling these results to give precedence
to security-centric bugs.

\subsection*{Visibility in Fuzzing}

The term `Visibility' relates to the amount of information collected about the
\acrshort{sut} that the fuzzer can access during runtime.
There exist three primary levels of visibility in
fuzzing: Blackbox~\cite{godefroid2007random}~\cite{manes2019art},
Graybox~\cite{canakci2021directfuzz}~\cite{li2018fuzzing}, and
Whitebox~\cite{godefroid2008automated}~\cite{godefroid2007random}~\cite{godefroid2008grammar} fuzzing,
each of them provides different levels of access to the source code of the \acrshort{sut}.

The combination of these methodologies—mutation-based~\cite{lyu2022ems}~\cite{miller2007analysis}
test case generation, coverage-guided~\cite{jaaskela2016genetic} execution and monitoring, in-depth bug analysis,
and the consideration of visibility levels—positions fuzzing as a formidable instrument
in bolstering the overall software quality and security. By diagnosing and
rectifying the root cause of software bugs and vulnerabilities, developers
can stymie the potential exploitation of these flaws by attackers.

The Figure:~\ref{fig:general_process_fuzzing} offers a visual representation of the
fuzzing process and its dependencies.

% \begin{figure}[ht]
% \centering
% \AltText{General Process Fuzzing}{\includegraphics[width=12.1cm][height=20.1cm]{general_process_fuzzing}}
% \caption{General Process Fuzzing\cite{liang2018fuzzing}.}\label{fig:general_process_fuzzing}
% \end{figure}

% \begin{figure}[ht]
% \centering
% \AltText{General Process Fuzzing}{\includegraphics[width=14.5cm, height=20.5cm]{general_process_fuzzing}}
% \caption{General Process Fuzzing\cite{liang2018fuzzing}}\label{fig:general_process_fuzzing}
% \end{figure}

\begin{figure}[h]
        \centering
        \AltText{Diagram illustrating the General Process of Fuzzing, which
        typically includes stages like target program identification, input generation,
        input execution, monitoring for exceptions or crashes, and logging outcomes.}
        {\adjustbox{width=\textwidth}{\includegraphics{general_process_fuzzing}}}
        \caption{General Process Fuzzing~\cite{liang2018fuzzing}}\label{fig:general_process_fuzzing}
\end{figure}

As the \gls{fuzz_target} of a fuzzing test, the \acrshort{sut}, typically a binary program,
is meticulously scrutinized by fuzzers. The generated inputs for the fuzzing process,
derived from either mutation-based or generation-based methods, comprise a mixture of valid and
invalid inputs that pass initial validation but may still trigger bugs. The process is dynamic
and continuously evolving, with ongoing research aimed at improving and automating the various
stages, from input generation to bug detection and filtering.
%\clearpage


% \section{Classification of Fuzzing Methods}\label{sec:fuzzing_methods}
% Fuzzing tools and methods are classified into three main categories: gray-box, white-box and
% black-box fuzzing.

% \subsection{White-Box Fuzzing Method}

% White-box fuzzing is a systematic technique for enumerating different interesting paths in a
% program by using program analysis and constraint solvers. It relies on the internal logic
% of the target program and is based on the technique called \textit{symbolic execution}\cite{cadar2013symbolic}.
% This method was proposed to overcome the limitations of black-box fuzzing and was first
% \Citeauthor{godefroid2008automated}. To scan through the target program, white-box fuzzing uses
% a search technique called coverage-maximizing heuristic search
% algorithm\cite{godefroid2008automated}\cite{liang2018fuzzing}.

% Before starting the fuzzing process, white-box fuzzing requires the necessary information
% from the target program to generate the inputs. It collects all the conditional paths of the target
% program by applying \acrlong{smt} formulas. For example, it uses the formula
% \begin{math}i[0] = 42 \land i[0] - i[1] > 7\end{math}, where \textit{i} is the set of inputs
% that traverse the target\cite{bohme2021fuzzing}. The path condition is then calculated and mutated
% and sent to a constraint solver to generate new paths and skip the blocks.
% The main goal of white-box fuzzers is to reach the maximum execution paths and requirements.


% One of the advantages of white-box fuzzing is that it can cover maximum coverage by generating
% better and more interesting test cases. However, numerous execution paths can lead to stability and
% compatibility issues in real-world software\cite{lomeli2022security}\cite{liang2018fuzzing}.
% Some existing white-box fuzzers are SAGE\cite{godefroid2012sage}, KLEE\cite{cadar2008klee},
% BitFuzz\cite{caballero2010input}.\newline

% \subsection{Black-Box Fuzzing Method}
% Black-box testing method involves generating inputs to test the target program without any
% internal knowledge or specifications. The fuzzing process begins by randomly mutating
% the valid inputs or generating new inputs. Examples of mutation data include flipping random bits
% and bytes in a seed file, byte copies, and removal\cite{jaaskela2016genetic}.
% Generational-based approaches utilize grammar and input-specification knowledge\cite{kim2013automatic}.

% The fuzzing process ends after a predefined time has been exhausted. Popular black-box fuzzers
% include Peach\cite{PeachFuz35:online}, Trinity\cite{GitHubke76:online} and
% Funfuzz\cite{GitHubMo73:online}.\newline

% \subsection{Gray-Box Fuzzing Method}
% The combination of black-box and white-box fuzzing utilizes \textit{code instrumentation}
% to provide feedback and obtain code coverage of the target program during runtime\cite{bohme2021fuzzing}.
% This approach employs genetic algorithm mutation strategies to generate new inputs which serve as
% new control locations to cover additional coverage paths. Feedback received from the coverage
% enables the fuzzers to reach more coverage, including tracing the taint data flow, in a method known
% as gray-box \textit{taint-analysis}\cite{bekrar2012taint}.

% To detect bugs and vulnerabilities, assertions are injected by sanitizers into the target program.
% Unlike the white-box method, the gray-box method utilizes runtime information to generate test cases.
% Some well-known gray-box fuzzers include AFL++\cite{257204}: a fork
% of \gls{afl}\cite{GitHubgo92:online}, LibFuzzer\cite{libFuzze17:online} and Honggfuzz\cite{GitHubgo89:online}.\newline

% \subsection{Choose a Fuzzing Method}
% In software testing, a `bug' refers to a defect in the software logic that causes unexpected or
% incorrect output. The primary objective of fuzz testing is to identify and eliminate such bugs
% in a system. In order to achieve this objective, selecting an appropriate fuzzing target is of
% utmost importance. While black-box fuzzers are simple and lightweight, they may not be capable
% of achieving high levels of code coverage and are more effective in uncovering `shallow`
% bugs. On the other hand, white-box and gray-box fuzzers are adept at detecting  `hidden' bugs
% that are difficult to identify. Although the implementation of these fuzzers is more complex
% and time-consuming, they offer comprehensive and in-depth testing capabilities.

% When considering fuzz testing methods, two important factors to consider are precision and efficiency,
% as well as the quality of the results. A `dumb' fuzzer simply mutates a valid input by
% randomly changing some bytes, while a `smart' fuzzer generates inputs from scratch.
% The black-box fuzzing approach is known for its precision and efficiency, while the white/gray
% box methods tend to produce higher quality results. Careful consideration of these factors is
% essential when selecting an appropriate fuzz testing method for a given software system.

% The two factors are:
% \begin{enumerate}
%         \item Time and Budget
%         \item Input format of the target program e.g, network protocol, compiler
% \end{enumerate}

% \section{Implementing the Art of Fuzzing}
% As per the section \hyperref[sec:fuzzing_methods]{Classification of Fuzzing Methods} below questions\cite{liang2018fuzzing}
% should be considered when building and implementing fuzzing with fuzzers.

% \begin{itemize}
%         \item How to do seed selection and generation.
%         \item What makes a good \gls{fuzz_target} and how to validate the inputs.
%         \item What to consider with the test cases which indicates crashes.
%         \item How to make good use of information during the runtime.
%         \item How can we achieve scalability improvements.\newline
% \end{itemize}

% \subsection{Generation and Selection of Seed}

% In the field of fuzz testing, the term \textit{seed corpus} denotes an assortment
% of representative input files that a target program is designed to process.
% The composition of the seed corpus can profoundly influence the efficacy and efficiency
% of the fuzzing process, emphasizing the importance of its selection for
% maximizing test coverage\cite{herrera2021seed}.

% Striving for high coverage implies the goal to activate and evaluate as
% many code paths as possible within the target program during the fuzzing process.
% While 100\% coverage represents a theoretical ideal, practically achieving this
% level of thoroughness is rare due to the intricate and expansive nature of
% modern software. Nevertheless, aiming for high coverage substantially enhances
% the probability of revealing concealed bugs and vulnerabilities, thus providing
% a comprehensive evaluation of the system's reliability and security\cite{godefroid2012sage}.

% The size of the seed corpus necessitates a careful balancing act between file
% size and coverage. While larger seed files may offer greater potential for
% coverage, they consume more computational resources and processing time. Smaller
% seed files, processed more rapidly, might not deliver the same level of coverage.
% Therefore, if a smaller seed file yields similar coverage to a larger one, its
% usage is preferable for improved testing
% efficiency\cite{liang2018fuzzing}\cite{jurczyk2016effective}. Nonetheless,
% in scenarios with ample storage capacity—either physical or cloud—larger seed
% files can be utilized, often compressed to conserve space without compromising
% the comprehensive representation of potential program inputs\cite{liang2018fuzzing}.

% The selection of the seed corpus typically precedes the fuzzing process,
% particularly crucial when employing a mutation-based fuzzer. This type of fuzzer
% requires an initial set of inputs or \textit{seed corpus} to commence testing.
% The quality of these initial seeds significantly influences the fuzzer's capacity
% to explore the program's state space and detect bugs\cite{miller2007analysis}.

% Moreover, a method for enhancing code coverage, as
% proposed by\Citeauthor{kim2011efficient}\cite{kim2011efficient},
% involves an in-depth analysis of binary file fields during fuzzing.
% This strategy entails tracking and evaluating stack frames, assembly codes,
% and registers to unearth potential new inputs that could augment coverage.

% In summary, the careful selection of a suitable seed corpus is a vital
% component of effective fuzz testing. The goal is to optimize coverage while
% considering the constraints of file size and storage capacity. Particularly in
% mutation-based fuzzing, a well-chosen seed corpus can lead to a more efficient and
% comprehensive exploration of the target program.


% \subsection{A Good Fuzz Target and Validation of Inputs}
% In the context of fuzz testing, the term `fuzz target' refers to the item being tested.
% This item can take many forms, such as a command line tool or a hardware device, and is
% characterized by its ability to accept inputs and provide results. The effectiveness of
% the fuzz testing process depends on the quality of the fuzz target and its ability to
% accurately represent the behavior of the real-world system or application being tested\cite{238602}.

% \subsubsection{A Good Fuzz Target}
% Below given is an example of a fuzz target written in C with \textit{LLVM libFuzzer}\cite{libFuzze17:online},
% \begin{minted}[linenos,frame=lines,baselinestretch=1.2,breaklines]{c}

% extern "C" int LLVMFuzzerTestOneInput (const uint8_t *Data, size_t Size) {
%         DoSomethingInterestingWithMyAPI(Data, Size);
%         return 0;  // Values other than 0 and -1 are reserved for future use.
% }

% \end{minted}

% The function \textit{LLVMFuzzerTestOneInput} of the fuzzer \textit{libFuzzer} takes two arguments,
% the address of the data to store and size. The API of the fuzz target is called in the next steps of
% fuzzing.

% It is crucial to keep the following aspects in mind when creating a fuzz target\cite{libFuzze17:online}\cite{257204}:

% \begin{itemize}
% \item The \gls{fuzzing_engine} needs to be designed so that it can execute the fuzz target multiple times within the same process, enhancing efficiency.
% \item The fuzz target should be able to handle a wide range of inputs, from valid to malformed and even empty inputs, increasing its robustness.
% \item An abrupt halt or unexpected exit from the fuzz target typically indicates a bug within the system. Thus, these instances warrant close monitoring.
% \item To expedite testing, the fuzz target's execution speed should be optimized. Furthermore, it should be designed to consume minimal memory, avoiding the risk of out-of-memory errors.
% \item If the fuzz target relies on a global state, it should be designed to avoid modifying it. For instance, the use of \textit{malloc()} could inadvertently modify the global state.
% \item To foster a culture of regular testing, the fuzz target should be integrated into a \textit{continuous integration} system.
% \item Consistent results are key in fuzzing. Consequently, the fuzz target should be deterministic, meaning that it should not rely on random factors that could lead to inconsistent outcomes.
% \item The fuzz target should be designed for speed as the fuzzing process requires multiple iterations. Therefore, its performance can significantly impact the testing speed.
% \item The memory consumption of the fuzz target should be kept minimal to avoid running into out-of-memory errors which could disrupt the testing process.
% \item It is essential to ensure the fuzz target is designed in a way that avoids conditions that could lead to timeouts and out-of-memory errors, thus ensuring smooth and uninterrupted testing.
% \end{itemize}

% \subsubsection{Input Validations}
% Fuzzing offers the main advantage of generating test cases quickly. However, when the fuzz target
% being tested has input validations, the generated test cases may not be useful. Hence, it is
% crucial to exercise caution when creating test cases in such scenarios.

% \textbf{Integrity Validation:}
% Integrity validation is a crucial aspect of maintaining data integrity in various inputs
% such as file formats and network protocols. To ensure the integrity of the data,
% checksum mechanisms are used, which provide validation at both the sending and receiving ends.
% In this regard, a fuzzer named \textit{TaintScope}, developed by \citeauthor{wang2010taintscope},
% is used to mutate the hot bytes in the input, which helps in generating new test cases. However, to pass the
% integrity validation, the checksum is reverted to its original state. This approach enables
% the detection of vulnerabilities that might arise in the process of transmitting data,
% which could compromise the integrity of the system\cite{wang2010taintscope}.

% \textbf{Format Validation:}
% Fuzz targets such as Android devices, compilers, and interpreters have format requirements that
% must be met by inputs in order to be accepted for testing. However, in cases where inputs
% do not conform to these formats, they are rejected by the system. To avoid format-based validation,
% a \textit{Grammar}-based solution can be employed. For instance, \Citeauthor{cao2015towards},
% developed a scanner for fuzz targets on Android devices, which is capable of evading the
% initial format validation and allowing the fuzz testing process to continue without
% interruption\cite{cao2015towards}.

% \textbf{Environment Validation:}
% Environment validation is a crucial aspect of testing, which checks the validity of configurations,
% runtime status, and other environmental factors. Fuzzing tools, such as FuzzDroid\cite{rasthofer2017making},
% utilize a combination of static and dynamic analysis to generate an Android runtime environment.
% The tool employs a search-based algorithm to identify potential vulnerabilities, which helps in
% enhancing the overall security of the system. Through this approach, FuzzDroid can effectively
% identify and address issues related to syntactic and semantic validity, ultimately improving the
% reliability and performance of the system.


% \textbf{Input Coverage:}
% Achieving high input coverage is critical to uncovering vulnerabilities in a target system.
% Various techniques have been proposed to improve the input coverage of a fuzzer.
% One such tool is the \textit{semi-valid coverage (SVCov)} tool, proposed by  \Citeauthor{tsankov2013semi},
% SVCov helps increase the existing input coverage by generating inputs that are close to
% valid inputs but still contain errors or faults that can trigger bugs in the system under test\cite{tsankov2013semi}.

% Another technique to enhance input coverage is through the use of specialized fuzzers such as \textit{Artfuzz},
% which was proposed by \Citeauthor{chen2016dynamically}. Artfuzz is designed to find non-crash buffer overflow
% vulnerabilities by identifying inputs that can trigger buffer overflows but do not cause the system to crash.
% By uncovering such vulnerabilities, Artfuzz helps improve the security of the target system\cite{chen2016dynamically}.

% \subsubsection{Isolating Crashes and Test Cases}
% Fuzzing often causes crashes in the target systems. These crashes can be huge in numbers and often
% takes a large amount of time for analyzing. Due to time and budget often the most priority bugs get
% fixed by the developers. Therefore, it is really important for the testers to isolate or filter
% the test inducing crashes and take the most useful test cases into account.

% \Citeauthor{chen2013taming} introduced a ranking based to isolate the crashes and test cases\cite{chen2013taming}.
% In this method, the test cases which can cause bugs, and they are different in nature are ranked accordingly.
% There are other methods for isolating the crashes such as clustering methods, differentiating the crashes based
% on their uniqueness and debug information. The stack traces play an important role in determining the uniqueness
% of the test cases and therefore need to recorded while doing the fuzzing. Another simpler way to determine the
% uniqueness is to trace the execution path. Fuzzers like AFL\cite{GitHubgo92:online} determines a crash is unique
% when it does not find the execution path. The uniqueness helps in isolating the crashes, the outputs and saves
% time when analyzing them. Trimming of the test cases is another method of isolating the crashes and test cases.
% A large size test case can take more time to execute and then more time for analysis. In mutation based fuzzers,
% in which the test cases are developed and mutated during the run time usually fall into this category. Hence, trimming
% of the test cases is needed from time to time while fuzzing and which can improve the overall efficiency. Trimming
% is usually done by removing the identical data-blocks which can not influenced the execution path anymore
% by comparing them with the original one.

% \subsubsection{Runtime Information During Fuzzing}
% To get code coverage and data flow which are runtime information,  symbolic execution and dynamic
% analysis techniques are used. Although these techniques help in finding the `hidden' bugs, these smart
% fuzzing techniques are low in efficiency.


% Path Explosion is one of the biggest problem in symbolic execution during fuzzing run time. A conditional branch
% in any target program can have a numerous execution paths and this can lead to Path Explosion.
% \Citeauthor{godefroid2007compositional} proposed to have \textit{function summaries} in low level
% functions. This can lead to less number of execution paths as the higher level functions can
% reuse them\cite{godefroid2007compositional}. Heuristic search algorithms such as random path selection
% and automatic partial loop summarization help to find the most relevant paths\cite{liang2018fuzzing}.

% Complex programs can cause imprecision symbolic execution. Methods such as CUTE\cite{sen2005cute} helps
% in making the symbolic execution more cost effective.

% During the runtime, \textit{Undertainting} is another problem where without any direct assignment The
% variable is transferred. \Citeauthor{kang2011dta++} has proposed that to Undertainting, it is essential to
% neglect the input data flow if the input is tainted\cite{kang2011dta++}.


% \subsubsection{Isolating Crashes and Test Cases}

% Fuzzing frequently leads to numerous system crashes, which can be cumbersome and
% time-consuming to analyze. As a result of time and budget constraints,
% prioritization becomes crucial; hence, testers must effectively isolate
% and filter crash-inducing test cases. \Citeauthor{chen2013taming} proposed a
% ranking-based method to categorize these cases based on their bug-inducing
% capabilities and their distinctiveness\cite{chen2013taming}.

% Isolation of crashes can also be achieved through clustering methods,
% differentiating crashes based on uniqueness and debug information. Stack
% traces and execution path tracing are essential in determining the uniqueness
% of the test cases. For instance, AFL uses the absence of a pre-existing execution
% path to define the uniqueness of a crash\cite{GitHubgo92:online}.

% Trimming is another approach employed to optimize the fuzzing process. Large
% test cases, often generated in mutation-based fuzzers, could potentially slow
% down execution and analysis. To counter this, identical data-blocks that no
% longer influence the execution path are removed periodically, thereby enhancing
% overall efficiency.

% \subsubsection{Runtime Information During Fuzzing}

% Smart fuzzing techniques like symbolic execution and dynamic analysis are used
% to gather runtime information such as code coverage and data flow, although
% their efficiency can be limited. One significant challenge is path explosion,
% a phenomenon caused by the numerous execution paths stemming from a single
% conditional branch in the target program.

% \Citeauthor{godefroid2007compositional} suggested the use of `function summaries'
% for lower-level functions to reduce the number of execution paths, allowing
% higher-level functions to reuse them\cite{godefroid2007compositional}.
% Additionally, heuristic search algorithms like random path selection and
% automatic partial loop summarization help to identify the most relevant paths\cite{liang2018fuzzing}.


\section{Implementing the Art of Fuzzing}
Based on the classification of fuzzing methods delineated in
section \hyperref[sec:fuzzing_methods]{Classification of Fuzzing Methods},
several questions arise when constructing and implementing fuzzing with
fuzzers~\cite{liang2018fuzzing}:

\begin{itemize}
\item How to select and generate \gls{seeds}?
\item What characteristics define a good \gls{fuzz_target}, and how can one validate the inputs?
\item What considerations must be considered when dealing with test cases causing crashes?
\item How to effectively utilize information during runtime?
\item How to achieve scalability improvements?
\end{itemize}
The responses to these queries determine the effectiveness of fuzzing in
revealing hidden vulnerabilities, thereby enhancing the security and reliability of the target system.

\subsection{Generation and Selection of Seed}
The \gls{seeds} corpus in fuzz testing refers to a collection of representative input
files that the target program is designed to process. Choosing the right seed
corpus is crucial as it greatly affects how effective and efficient the fuzz
testing process is, underlining its significance in achieving maximum test
coverage~\cite{herrera2021seed}.

In an ideal scenario, a fuzzer aims for 100\% coverage, seeking to activate and
examine as many code paths as possible within the target program. However, due
to the complexity achieving high coverage is
often difficult. Still, aiming for high coverage greatly increases the
likelihood of finding hidden bugs and vulnerabilities, which helps in getting
the program's reliability and security~\cite{godefroid2012sage}.

The size of the seed corpus must be chosen with careful consideration of
its potential effect on coverage and computational resources. Larger seed files
may provide more coverage but consume more resources and time. If smaller seed
files can provide similar coverage to larger ones, they are typically preferred
for efficient testing~\cite{liang2018fuzzing}~\cite{jurczyk2016effective}.

The initial seed corpus is particularly crucial when using a mutation-based
fuzzer, which requires an initial set of \gls{seeds}. The quality of these
\gls{seeds} substantially influences the fuzzer's capacity to explore the program's
state space and detect bugs~\cite{miller2007analysis}. An efficient way of
enhancing code coverage, proposed by~\Citeauthor{kim2011efficient}, involves a
detailed analysis of binary file fields during fuzzing. This approach includes
tracking and evaluating stack frames, assembly codes, and registers to unearth
potential new inputs that could augment coverage~\cite{kim2011efficient}.

\subsection{A Good Fuzz Target and Validation of Inputs}
In the context of fuzz testing, the `fuzz target' is the item being tested.
This can be anything from a command line tool to a hardware device. The quality
of the \gls{fuzz_target} is crucial as it should accurately represent the behavior of
the real-world system or application being tested~\cite{238602}.

\subsubsection{A Good Fuzz Target}

A \gls{fuzz_target} using LLVM's libFuzzer~\cite{libFuzze17:online}:

\begin{minted}[linenos,frame=lines,baselinestretch=1.2,breaklines]{c}

extern "C" int LLVMFuzzerTestOneInput (const uint8_t *Data, size_t Size) {
DoSomethingInterestingWithMyAPI(Data, Size);
return 0; // Values other than 0 and -1 are reserved for future use.
}

\end{minted}

In this case, the function \textit{LLVMFuzzerTestOneInput} takes two arguments,
the address of the data to store and its size. The API of the fuzz target is
called in subsequent steps of fuzzing. When developing a fuzz target, given
key aspects should be considered.~\cite{libFuzze17:online}~\cite{257204}:

\begin{itemize}
\item The \gls{fuzzing_engine} should be designed to execute the fuzz target
multiple times within the same process to enhance efficiency.
\item The fuzz target should handle a wide range of inputs, from valid to
malformed and even empty, to increase robustness.
\item Any abrupt halt or unexpected exit from the fuzz target usually
signifies a bug within the system and warrants closer examination.
\item For faster testing, the fuzz target's execution speed should be
optimized. Moreover, it should consume minimal memory to avoid out-of-memory errors.
\item If the fuzz target relies on a global state, it should be
designed to avoid altering it.
\item Regular testing should be encouraged by integrating the fuzz
target into a continuous integration system.
\item The fuzz target should be deterministic to ensure consistent results.
\item It is crucial to optimize the performance of the fuzz target as the
fuzzing process requires multiple iterations.
\end{itemize}

\subsubsection{Isolating Crashes and Test Cases}
Fuzz testing frequently leads to numerous system crashes, which can be difficult
and time-consuming to analyze. Given time and budget limitations, it's crucial
to efficiently isolate and sort out test cases that cause crashes. A method~\cite{chen2013taming}
for ranking these cases, based on their ability to induce bugs and their uniqueness,
has been suggested by~\Citeauthor{chen2013taming}.

Crashes can also be isolated through clustering methods, differentiating
crashes based on uniqueness and debug information. For example, AFL uses the absence
of a pre-existing execution path to define the uniqueness of a crash~\cite{GitHubgo92:online}.

\subsubsection{Runtime Information During Fuzzing}
Advanced fuzz testing methods, like symbolic execution and dynamic analysis,
collect runtime information including code coverage and data flow. However,
their effectiveness can be hindered by issues like path explosion. This issue
arises from the numerous execution paths that originate from a single
conditional branch in the program being tested.

\Citeauthor{godefroid2007compositional} suggested using `function summaries'
for lower-level functions to reduce the number of execution paths. This allows
higher-level functions to reuse them~\cite{godefroid2007compositional}. Additionally,
heuristic search algorithms like random path selection and automatic partial
loop summarization help identify the most relevant paths~\cite{liang2018fuzzing}.

In summary, effective fuzzing implementation requires careful consideration of
various factors. The seed selection and generation, quality of the fuzz target,
test case management, use of runtime information, and scalability all play
significant roles in the process. Employing advanced techniques and maintaining
a clear focus on these aspects can optimize the fuzzing process and enhance its
ability to uncover hidden bugs and vulnerabilities in the target system.
\clearpage